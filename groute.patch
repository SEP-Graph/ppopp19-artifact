diff --git a/CMakeLists.txt b/CMakeLists.txt
index 6e996b2..3bab547 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -88,6 +88,12 @@ target_link_libraries(pr ${EXTRA_LIBS})
 cuda_add_executable(pbf samples/pbf/pbf_async.cu samples/pbf/main.cpp)
 target_link_libraries(pbf ${EXTRA_LIBS})
 
+cuda_add_executable(bc src/utils/parser.cpp src/utils/utils.cpp src/groute/graphs/csr_graph.cpp
+        samples/bc/bc_async.cu
+        samples/bc/bc_host.cpp
+        samples/bc/main.cpp)
+target_link_libraries(bc ${EXTRA_LIBS})
+
 
 # Unit tests
 enable_testing()
diff --git a/include/groute/worklist_stack.h b/include/groute/worklist_stack.h
new file mode 100644
index 0000000..d5ccc11
--- /dev/null
+++ b/include/groute/worklist_stack.h
@@ -0,0 +1,264 @@
+//
+// Created by liang on 7/25/18.
+//
+
+#ifndef HYBRID_WORKLIST_STACK_H
+#define HYBRID_WORKLIST_STACK_H
+
+#include <groute/graphs/traversal_algo.h>
+#include <cub/grid/grid_barrier.cuh>
+
+namespace groute
+{
+    namespace dev
+    {
+        template<typename T>
+        struct WorklistStack
+        {
+            uint32_t *m_stack;
+            T *m_data;
+            uint32_t *m_stack_depth;
+            uint32_t *m_data_pos;
+            uint32_t m_max_depth;
+            uint32_t m_capacity;
+
+            __host__ __device__
+
+            WorklistStack(uint32_t *stack, T *data, uint32_t *stack_depth, uint32_t *data_pos, uint32_t max_depth, uint32_t capacity) :
+                    m_stack(stack), m_data(data), m_stack_depth(stack_depth), m_data_pos(data_pos), m_max_depth(max_depth), m_capacity(capacity)
+            {
+#if defined(__CUDA_ARCH__)
+                assert(*m_stack_depth == 0);
+                assert(*m_data_pos == 0);
+#endif
+                assert(m_max_depth > 0);
+                assert(m_capacity > 0);
+            }
+
+            __device__ __forceinline__
+
+            void append(T item)
+            {
+                uint32_t last_pos = atomicAdd(m_data_pos, 1);
+
+                assert(last_pos < m_capacity);
+                m_data[last_pos] = item;
+            }
+
+            __device__ __forceinline__
+
+            void push()
+            {
+                assert(*m_stack_depth < m_max_depth);
+
+                *m_stack_depth += 1;
+                m_stack[*m_stack_depth] = *m_data_pos;
+            }
+
+            __device__ __forceinline__
+
+            void reset()
+            {
+                *m_stack_depth = 0;
+                *m_data_pos = 0;
+            }
+
+            __device__ __forceinline__
+
+            uint32_t begin_pos(uint32_t depth) const
+            {
+                assert(depth >= 0);
+                assert(depth < *m_stack_depth);
+
+                return m_stack[depth];
+            }
+
+            __device__ __forceinline__
+
+            uint32_t end_pos(uint32_t depth) const
+            {
+                assert(depth >= 0);
+                assert(depth + 1 <= *m_stack_depth);
+
+                return m_stack[depth + 1];
+            }
+
+            __device__ __forceinline__
+
+            T read(uint32_t pos) const
+            {
+                assert(pos >= 0);
+                assert(pos < *m_data_pos);
+
+                return m_data[pos];
+            }
+
+            __device__ __forceinline__
+
+            uint32_t get_depth() const
+            {
+                return *m_stack_depth;
+            }
+
+            __device__ __forceinline__
+
+            uint32_t count() const
+            {
+                return *m_data_pos;
+            }
+
+            __device__ __forceinline__
+
+            uint32_t count(uint32_t depth) const
+            {
+                assert(depth >= 0);
+                assert(depth + 1 <= *m_stack_depth);
+
+                return m_stack[depth + 1] - m_stack[depth];
+            }
+        };
+
+        template<typename T>
+        __global__ void Pushback(WorklistStack<T> worklist_stack,
+                                 T *p_items,
+                                 uint32_t len)
+        {
+            uint32_t tid = TID_1D;
+            uint32_t nthreads = TOTAL_THREADS_1D;
+            uint32_t s_top = *worklist_stack.m_stack_depth;
+            uint32_t last_pos = worklist_stack.m_stack[s_top];
+
+            if (tid == 0)
+            {
+                assert(last_pos + len <= worklist_stack.m_capacity);
+                assert(last_pos == *worklist_stack.m_data_pos);
+            }
+
+            for (uint32_t idx = tid; idx < len; idx += nthreads)
+            {
+                worklist_stack.m_data[last_pos + idx] = p_items[idx];
+            }
+        }
+
+        // We need a barrier for Pushback function
+        template<typename T>
+        __global__ void Commit(WorklistStack<T> worklist_stack, uint32_t len)
+        {
+            uint32_t tid = TID_1D;
+
+            if (tid == 0)
+            {
+                uint32_t s_top = *worklist_stack.m_stack_depth;
+                uint32_t last_pos = worklist_stack.m_stack[s_top];
+
+                *worklist_stack.m_data_pos += len;
+                *worklist_stack.m_stack_depth += 1;
+                worklist_stack.m_stack[*worklist_stack.m_stack_depth] = last_pos + len;
+            }
+        }
+    }
+
+
+    template<typename T>
+    class WorklistStack
+    {
+        uint32_t *m_stack;
+        T *m_data;
+        uint32_t *m_stack_depth;
+        uint32_t *m_data_pos;
+        uint32_t m_max_depth;
+        uint32_t m_capacity;
+    public:
+        WorklistStack(uint32_t capacity, uint32_t max_depth = 100000) :
+                m_capacity(capacity), m_max_depth(max_depth)
+        {
+            assert(m_max_depth > 0);
+            assert(m_capacity > 0);
+
+            GROUTE_CUDA_CHECK(cudaMalloc((void **) &m_stack, sizeof(uint32_t) * (m_max_depth + 1)));
+            GROUTE_CUDA_CHECK(cudaMalloc((void **) &m_data, sizeof(T) * m_capacity));
+            GROUTE_CUDA_CHECK(cudaMalloc((void **) &m_stack_depth, sizeof(uint32_t)));
+            GROUTE_CUDA_CHECK(cudaMalloc((void **) &m_data_pos, sizeof(uint32_t)));
+            GROUTE_CUDA_CHECK(cudaMemset(m_stack_depth, 0, sizeof(uint32_t)));
+            GROUTE_CUDA_CHECK(cudaMemset(m_data_pos, 0, sizeof(uint32_t)));
+        }
+
+        typedef dev::WorklistStack<T> DeviceObjectType;
+
+        DeviceObjectType DeviceObject() const
+        {
+            return dev::WorklistStack<T>(m_stack,
+                                         m_data,
+                                         m_stack_depth,
+                                         m_data_pos,
+                                         m_max_depth,
+                                         m_capacity);
+        }
+
+        void ResetAsync(cudaStream_t stream)
+        {
+            GROUTE_CUDA_CHECK(cudaMemset(m_stack_depth, 0, sizeof(uint32_t)));
+            GROUTE_CUDA_CHECK(cudaMemset(m_data_pos, 0, sizeof(uint32_t)));
+        }
+
+        void ResetAsync(const groute::Stream &stream)
+        {
+            ResetAsync(stream.cuda_stream);
+        }
+
+        void PushAsync(T *data_ptr, uint32_t len, const groute::Stream &stream)
+        {
+            dim3 grid_dims, block_dims;
+
+            KernelSizing(grid_dims, block_dims, len);
+
+            dev::Pushback << < grid_dims, block_dims, 0, stream.cuda_stream >> > (DeviceObject(), data_ptr, len);
+            dev::Commit << < 1, 1, 0, stream.cuda_stream >> > (DeviceObject(), len);
+        }
+
+        T *GetDataPtr(uint32_t depth)
+        {
+            uint32_t begin_pos;
+
+            GROUTE_CUDA_CHECK(cudaMemcpy(&begin_pos, m_stack + depth, sizeof(uint32_t), cudaMemcpyDeviceToHost));
+            return m_data + begin_pos;
+        }
+
+        uint32_t GetDepth(const groute::Stream &stream) const
+        {
+            uint32_t depth;
+
+            GROUTE_CUDA_CHECK(cudaMemcpy(&depth, m_stack_depth, sizeof(uint32_t), cudaMemcpyDeviceToHost));
+
+            return depth;
+        }
+
+        uint32_t GetCount(const groute::Stream &stream) const
+        {
+            uint32_t count;
+
+            GROUTE_CUDA_CHECK(cudaMemcpy(&count, m_data_pos, sizeof(uint32_t), cudaMemcpyDeviceToHost));
+
+            return count;
+        }
+
+        uint32_t GetCount(uint32_t depth, const groute::Stream &stream) const
+        {
+            uint32_t begin_pos, end_pos;
+
+            GROUTE_CUDA_CHECK(cudaMemcpy(&begin_pos, m_stack + depth, sizeof(uint32_t), cudaMemcpyDeviceToHost));
+            GROUTE_CUDA_CHECK(cudaMemcpy(&end_pos, m_stack + depth + 1, sizeof(uint32_t), cudaMemcpyDeviceToHost));
+
+            return end_pos - begin_pos;
+        }
+
+        ~WorklistStack()
+        {
+            GROUTE_CUDA_CHECK(cudaFree(m_stack));
+            GROUTE_CUDA_CHECK(cudaFree(m_data));
+            GROUTE_CUDA_CHECK(cudaFree(m_stack_depth));
+            GROUTE_CUDA_CHECK(cudaFree(m_data_pos));
+        }
+    };
+}
+#endif //HYBRID_WORKLIST_STACK_H
diff --git a/samples/bc/bc_async.cu b/samples/bc/bc_async.cu
new file mode 100644
index 0000000..a411107
--- /dev/null
+++ b/samples/bc/bc_async.cu
@@ -0,0 +1,593 @@
+// Groute: An Asynchronous Multi-GPU Programming Framework
+// http://www.github.com/groute/groute
+// Copyright (c) 2017, A. Barak
+// All rights reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+// * Redistributions of source code must retain the above copyright notice,
+//   this list of conditions and the following disclaimer.
+// * Redistributions in binary form must reproduce the above copyright notice,
+//   this list of conditions and the following disclaimer in the documentation
+//   and/or other materials provided with the distribution.
+// * Neither the names of the copyright holders nor the names of its 
+//   contributors may be used to endorse or promote products derived from this
+//   software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
+#include <vector>
+#include <algorithm>
+#include <thread>
+#include <memory>
+#include <random>
+
+#include <gflags/gflags.h>
+
+#include <groute/event_pool.h>
+#include <groute/distributed_worklist.h>
+#include <groute/worklist_stack.h>
+#include <utils/parser.h>
+#include <utils/utils.h>
+#include <utils/stopwatch.h>
+#include <utils/markers.h>
+
+#include <groute/graphs/csr_graph.h>
+#include <groute/graphs/traversal_algo.h>
+#include <groute/cta_work.h>
+#include <utils/cuda_utils.h>
+
+#include "bc_common.h"
+
+DEFINE_int32(source_node, 0, "The source node for the BC traversal (clamped to [0, nnodes-1])");
+
+const level_t INF = UINT_MAX;
+
+#define GTID (blockIdx.x * blockDim.x + threadIdx.x)
+
+
+namespace bc
+{
+    __global__ void BCInit(level_t *levels,
+                           sigma_t *sigmas,
+                           int nnodes,
+                           index_t source)
+    {
+        int tid = GTID;
+        if (tid < nnodes)
+        {
+            if (tid == source)
+            {
+                levels[tid] = 0;
+                sigmas[tid] = 1;
+            }
+            else
+            {
+                levels[tid] = INF;
+                sigmas[tid] = 0;
+            }
+        }
+    }
+
+
+    template<typename TGraph,
+            typename TGraphDatum,
+            typename TWorklist,
+            typename TWLStack>
+    __global__ void BFSKernelFused(TGraph graph,
+                                   TGraphDatum levels_datum,
+                                   sigma_t *p_node_sigmas,
+                                   index_t *p_search_depth,
+                                   TWorklist wl1,
+                                   TWorklist wl2,
+                                   TWLStack wl_stack,
+                                   cub::GridBarrier grid_barrier)
+    {
+        int tid = TID_1D;
+        unsigned nthreads = TOTAL_THREADS_1D;
+        uint32_t work_size;
+        TWorklist *wl_in = &wl1;
+        TWorklist *wl_out = &wl2;
+
+        while ((work_size = wl_in->len()) > 0)
+        {
+            for (uint32_t i = 0 + tid; i < work_size; i += nthreads)
+            {
+                index_t node = wl_in->read(i);
+                level_t next_level = levels_datum.get_item(node) + 1;
+
+                wl_stack.append(node);
+
+                for (index_t edge = graph.begin_edge(node), end_edge = graph.end_edge(node); edge < end_edge; ++edge)
+                {
+                    index_t dest = graph.edge_dest(edge);
+                    level_t prev = atomicMin(levels_datum.get_item_ptr(dest), next_level);
+
+                    if (prev == INF)
+                    {
+                        atomicAdd(p_node_sigmas + dest, p_node_sigmas[node]);
+                        atomicMax(p_search_depth, next_level);
+                    }
+                    else
+                    {
+                        if (levels_datum[dest] == next_level)
+                        {
+                            atomicAdd(p_node_sigmas + dest, p_node_sigmas[node]);
+                        }
+                    }
+
+                    if (next_level < prev)
+                    {
+                        wl_out->append(dest);
+                    }
+                }
+            }
+            grid_barrier.Sync();
+            if (tid == 0)
+            {
+                wl_in->reset();
+                wl_stack.push();
+            }
+            grid_barrier.Sync();
+            auto *tmp = wl_in;
+            wl_in = wl_out;
+            wl_out = tmp;
+        }
+    }
+
+    template<typename TGraph,
+            typename TGraphDatum,
+            typename TWorklist,
+            typename TWLStack>
+    __global__ void BFSKernelCTAFused(TGraph graph,
+                                      TGraphDatum levels_datum,
+                                      sigma_t *p_node_sigmas,
+                                      index_t *p_search_depth,
+                                      TWorklist wl1,
+                                      TWorklist wl2,
+                                      TWLStack wl_stack,
+                                      cub::GridBarrier grid_barrier)
+    {
+        int tid = TID_1D;
+        unsigned nthreads = TOTAL_THREADS_1D;
+        uint32_t work_size;
+        TWorklist *wl_in = &wl1;
+        TWorklist *wl_out = &wl2;
+
+        while ((work_size = wl_in->len()) > 0)
+        {
+            uint32_t work_size_rup = round_up(work_size, blockDim.x) * blockDim.x;
+
+            for (uint32_t i = 0 + tid; i < work_size_rup; i += nthreads)
+            {
+                groute::dev::np_local<index_t> np_local = {0, 0, 0};
+
+                if (i < work_size)
+                {
+                    index_t node = wl_in->read(i);
+
+
+                    wl_stack.append(node);
+
+                    np_local.start = graph.begin_edge(node);
+                    np_local.size = graph.end_edge(node) - np_local.start;
+                    np_local.meta_data = node;
+                }
+
+                groute::dev::CTAWorkScheduler<level_t>::schedule
+                        (np_local, [&graph, &levels_datum, &p_node_sigmas, &p_search_depth, &wl_out](index_t edge,
+                                                                                                     index_t node)
+                        {
+                            level_t next_level = levels_datum.get_item(node) + 1;
+                            index_t dest = graph.edge_dest(edge);
+                            level_t prev = atomicMin(levels_datum.get_item_ptr(dest), next_level);
+
+                            if (prev == INF)
+                            {
+                                atomicAdd(p_node_sigmas + dest, p_node_sigmas[node]);
+                                atomicMax(p_search_depth, next_level);
+                            }
+                            else
+                            {
+                                if (levels_datum[dest] == next_level)
+                                {
+                                    atomicAdd(p_node_sigmas + dest, p_node_sigmas[node]);
+                                }
+                            }
+
+                            if (next_level < prev)
+                            {
+                                wl_out->append(dest);
+                            }
+                        });
+            }
+            grid_barrier.Sync();
+            if (tid == 0)
+            {
+                wl_in->reset();
+                wl_stack.push();
+            }
+            grid_barrier.Sync();
+            auto *tmp = wl_in;
+            wl_in = wl_out;
+            wl_out = tmp;
+        }
+    }
+
+    template<typename Graph,
+            typename WLStack,
+            typename SourcePath,
+            typename Sigmas>
+    __global__ void StageTwoDDFused(Graph graph,
+                                    WLStack wl_stack,
+                                    SourcePath node_source_path,
+                                    Sigmas *p_node_sigmas,
+                                    Sigmas *p_node_bc_values,
+                                    uint32_t *p_search_depth,
+                                    cub::GridBarrier barrier)
+    {
+        uint32_t tid = TID_1D;
+        uint32_t nthreads = TOTAL_THREADS_1D;
+        uint32_t curr_depth = *p_search_depth;
+
+        while (curr_depth > 0)
+        {
+            uint32_t begin_pos = wl_stack.begin_pos(curr_depth);
+            uint32_t end_pos = wl_stack.end_pos(curr_depth);
+
+            for (uint32_t idx = tid + begin_pos; idx < end_pos; idx += nthreads)
+            {
+                index_t node = wl_stack.read(idx);
+                index_t src_depth = node_source_path[node];
+
+                for (index_t edge = graph.begin_edge(node), end_edge = graph.end_edge(node);
+                     edge < end_edge; edge++)
+                {
+                    index_t dest = graph.edge_dest(edge);
+
+                    if (node_source_path[dest] == src_depth + 1)
+                    {
+                        float delta_to = 1.0f * p_node_sigmas[node] / p_node_sigmas[dest] * (1.0f + p_node_bc_values[dest]);
+
+                        atomicAdd(p_node_bc_values + node, delta_to);
+                    }
+                }
+            }
+            barrier.Sync();
+            curr_depth--;
+        }
+    }
+
+
+    template<typename Graph,
+            typename WLStack,
+            typename SourcePath,
+            typename Sigmas>
+    __global__ void StageTwoDDCTAFused(Graph graph,
+                                       WLStack wl_stack,
+                                       SourcePath node_source_path,
+                                       Sigmas *p_node_sigmas,
+                                       Sigmas *p_node_bc_values,
+                                       uint32_t *p_search_depth,
+                                       cub::GridBarrier barrier)
+    {
+        uint32_t tid = TID_1D;
+        uint32_t nthreads = TOTAL_THREADS_1D;
+        uint32_t curr_depth = *p_search_depth;
+
+        while (curr_depth > 0)
+        {
+            uint32_t begin_pos = wl_stack.begin_pos(curr_depth);
+            uint32_t end_pos = wl_stack.end_pos(curr_depth);
+            uint32_t work_size = end_pos - begin_pos;
+            uint32_t work_size_rup = round_up(work_size, blockDim.x) * blockDim.x;
+
+            for (uint32_t i = 0 + tid; i < work_size_rup; i += nthreads)
+            {
+                groute::dev::np_local<index_t> np_local = {0, 0};
+
+                if (i < work_size)
+                {
+                    index_t node = wl_stack.read(begin_pos + i);
+
+                    np_local.start = graph.begin_edge(node);
+                    np_local.size = graph.end_edge(node) - np_local.start;
+                    np_local.meta_data = node;
+                }
+
+                groute::dev::CTAWorkScheduler<index_t>::schedule
+                        (np_local,
+                         [&graph, &node_source_path, &p_node_bc_values, &p_node_sigmas](index_t edge,
+                                                                                    index_t node)
+                         {
+                             index_t src_depth = node_source_path[node];
+                             index_t dest = graph.edge_dest(edge);
+
+                             if (node_source_path[dest] == src_depth + 1)
+                             {
+                                 float delta_to = 1.0f * p_node_sigmas[node] / p_node_sigmas[dest] *
+                                                  (1.0f + p_node_bc_values[dest]);
+
+                                 atomicAdd(p_node_bc_values + node, delta_to);
+                             }
+                             return true;
+                         });
+            }
+            barrier.Sync();
+            curr_depth--;
+        }
+    }
+
+    template<typename TGraph, typename TGraphDatum>
+    class Problem
+    {
+    private:
+        TGraph m_graph;
+        TGraphDatum m_levels_datum;
+        sigma_t *m_p_sigmas_datum;
+        centrality_t *m_p_bc_value_datum;
+        uint32_t *m_search_depth;
+    public:
+        Problem(const TGraph &graph,
+                const TGraphDatum &levels_datum,
+                sigma_t *p_sigmas_datum,
+                centrality_t *p_bc_value_datum,
+                uint32_t *search_depth) :
+                m_graph(graph), m_levels_datum(levels_datum),
+                m_p_sigmas_datum(p_sigmas_datum),
+                m_p_bc_value_datum(p_bc_value_datum),
+                m_search_depth(search_depth)
+        {
+        }
+
+        void Init(index_t source_node, groute::Worklist<index_t> &in_wl, groute::Stream &stream) const
+        {
+            dim3 grid_dims, block_dims;
+
+            KernelSizing(grid_dims, block_dims, m_levels_datum.size);
+
+            BCInit << < grid_dims, block_dims, 0, stream.cuda_stream >> > (m_levels_datum.data_ptr, m_p_sigmas_datum, m_graph.nnodes, source_node);
+            in_wl.AppendItemAsync(stream.cuda_stream, source_node);
+        }
+
+        template<typename TWorklist, typename TWLStack>
+        void Relax(TWorklist &wl1,
+                   TWorklist &wl2,
+                   TWLStack &wl_stack,
+                   groute::Stream &stream)
+        {
+            dim3 grid_dims, block_dims;
+            int occupancy_per_MP;
+            cudaDeviceProp dev_props;
+            cub::GridBarrierLifetime barrier;
+
+            GROUTE_CUDA_CHECK(cudaGetDeviceProperties(&dev_props, 0));
+
+            Stopwatch sw_stage1(true);
+            if (FLAGS_cta_np)
+            {
+                cudaOccupancyMaxActiveBlocksPerMultiprocessor(&occupancy_per_MP,
+                                                              BFSKernelCTAFused<groute::graphs::dev::CSRGraph,
+                                                                      groute::graphs::dev::GraphDatum<level_t>,
+                                                                      groute::dev::Worklist<index_t>,
+                                                                      groute::dev::WorklistStack<index_t >>, FLAGS_block_size, 0);
+
+                int fused_work_blocks = dev_props.multiProcessorCount * occupancy_per_MP;
+
+                grid_dims.x = fused_work_blocks;
+                block_dims.x = FLAGS_block_size;
+
+                cub::GridBarrierLifetime barrier;
+                barrier.Setup(grid_dims.x);
+
+                Stopwatch sw_stage1(true);
+
+                BFSKernelCTAFused << < grid_dims, block_dims, 0, stream.cuda_stream >> > (m_graph,
+                        m_levels_datum,
+                        m_p_sigmas_datum,
+                        m_search_depth,
+                        wl1.DeviceObject(),
+                        wl2.DeviceObject(),
+                        wl_stack.DeviceObject(),
+                        barrier);
+                stream.Sync();
+            }
+            else
+            {
+                cudaOccupancyMaxActiveBlocksPerMultiprocessor(&occupancy_per_MP,
+                                                              BFSKernelFused<groute::graphs::dev::CSRGraph,
+                                                                      groute::graphs::dev::GraphDatum<level_t>,
+                                                                      groute::dev::Worklist<index_t>,
+                                                                      groute::dev::WorklistStack<index_t >>, FLAGS_block_size, 0);
+
+                int fused_work_blocks = dev_props.multiProcessorCount * occupancy_per_MP;
+
+                grid_dims.x = fused_work_blocks;
+                block_dims.x = FLAGS_block_size;
+
+                barrier.Setup(grid_dims.x);
+
+                BFSKernelFused << < grid_dims, block_dims, 0, stream.cuda_stream >> > (m_graph,
+                        m_levels_datum,
+                        m_p_sigmas_datum,
+                        m_search_depth,
+                        wl1.DeviceObject(),
+                        wl2.DeviceObject(),
+                        wl_stack.DeviceObject(),
+                        barrier);
+                stream.Sync();
+            }
+
+            sw_stage1.stop();
+
+            printf("Time stage1: %f\n", sw_stage1.ms());
+
+            Stopwatch sw_stage2(true);
+
+            if (FLAGS_cta_np)
+            {
+                cudaOccupancyMaxActiveBlocksPerMultiprocessor(&occupancy_per_MP,
+                                                              StageTwoDDCTAFused<groute::graphs::dev::CSRGraph,
+                                                                      groute::dev::WorklistStack<index_t>,
+                                                                      groute::graphs::dev::GraphDatum<level_t>,
+                                                                      sigma_t>, FLAGS_block_size, 0);
+
+                grid_dims.x = dev_props.multiProcessorCount * occupancy_per_MP;
+
+                barrier.Setup(grid_dims.x);
+
+                StageTwoDDCTAFused << < grid_dims, block_dims, 0, stream.cuda_stream >> > (m_graph,
+                        wl_stack.DeviceObject(),
+                        m_levels_datum,
+                        m_p_sigmas_datum,
+                        m_p_bc_value_datum,
+                        m_search_depth,
+                        barrier);
+                stream.Sync();
+            }
+            else
+            {
+                cudaOccupancyMaxActiveBlocksPerMultiprocessor(&occupancy_per_MP,
+                                                              StageTwoDDFused<groute::graphs::dev::CSRGraph,
+                                                                      groute::dev::WorklistStack<index_t>,
+                                                                      groute::graphs::dev::GraphDatum<level_t>,
+                                                                      sigma_t>, FLAGS_block_size, 0);
+
+                grid_dims.x = dev_props.multiProcessorCount * occupancy_per_MP;
+
+                barrier.Setup(grid_dims.x);
+
+                StageTwoDDFused << < grid_dims, block_dims, 0, stream.cuda_stream >> > (m_graph,
+                        wl_stack.DeviceObject(),
+                        m_levels_datum,
+                        m_p_sigmas_datum,
+                        m_p_bc_value_datum,
+                        m_search_depth,
+                        barrier);
+                stream.Sync();
+            }
+
+            sw_stage2.stop();
+
+            printf("Time stage2: %f\n", sw_stage2.ms());
+        }
+
+
+    };
+
+    struct Algo
+    {
+        static const char *NameLower()
+        { return "bc"; }
+
+        static const char *Name()
+        { return "BC"; }
+
+    };
+}
+
+bool TestBCSingle()
+{
+    groute::graphs::single::NodeOutputDatum<level_t> levels_datum;
+    groute::graphs::traversal::Context<bc::Algo> context(1);
+    groute::graphs::single::CSRGraphAllocator dev_graph_allocator(context.host_graph);
+
+    context.SetDevice(0);
+
+    dev_graph_allocator.AllocateDatumObjects(levels_datum);
+
+    context.SyncDevice(0); // graph allocations are on default streams, must sync device
+
+    index_t nnodes = context.nvtxs;
+
+    utils::SharedArray<sigma_t> dev_node_sigmas(nnodes);
+    utils::SharedArray<float> dev_node_bc_values(nnodes);
+    utils::SharedValue<uint32_t> dev_search_depth;
+
+    bc::Problem<groute::graphs::dev::CSRGraph,
+            groute::graphs::dev::GraphDatum<level_t>> problem(dev_graph_allocator.DeviceObject(),
+                                                              levels_datum.DeviceObject(),
+                                                              dev_node_sigmas.dev_ptr,
+                                                              dev_node_bc_values.dev_ptr,
+                                                              dev_search_depth.dev_ptr);
+
+    size_t max_work_size = context.host_graph.nedges * FLAGS_wl_alloc_factor;
+    if (FLAGS_wl_alloc_abs > 0)
+        max_work_size = FLAGS_wl_alloc_abs;
+
+    groute::Stream stream;
+
+    groute::Worklist<index_t> wl1(max_work_size), wl2(max_work_size);
+    groute::WorklistStack<index_t> wl_stack(max_work_size * 2);
+
+    wl1.ResetAsync(stream.cuda_stream);
+    wl2.ResetAsync(stream.cuda_stream);
+    wl_stack.ResetAsync(stream);
+    stream.Sync();
+
+    index_t source_node = min(max(0, FLAGS_source_node), context.nvtxs - 1);
+
+    Stopwatch sw(true);
+
+    problem.Init(source_node, wl1, stream);
+    problem.Relax(wl1, wl2, wl_stack, stream);
+
+    stream.Sync();
+
+    sw.stop();
+
+    printf("\n%s: %f ms. <filter>\n\n", bc::Algo::Name(), sw.ms());
+
+    dev_node_sigmas.D2H();
+    dev_node_bc_values.D2H();
+
+    for (int i = 0; i < dev_node_bc_values.host_vec.size(); i++)
+    {
+        dev_node_bc_values.host_vec[i] /= 2;
+    }
+
+//    for (int i = 0; i < 100; i++)
+//    {
+//        printf("node: %d %f %f\n", i, dev_node_sigmas.host_vec[i], dev_node_bc_values.host_vec[i]);
+//    }
+    // Gather
+
+    if (FLAGS_output.length() != 0)
+        BCOutput(FLAGS_output.c_str(), dev_node_bc_values.host_vec);
+
+    if (FLAGS_check)
+    {
+        auto result_pair = BetweennessCentralityHost(context.host_graph, source_node);
+
+        int failed_sigmas = BCCheckErrors(result_pair.second, dev_node_sigmas.host_vec);
+
+        if (failed_sigmas)
+        {
+            printf("Sigams failed!\n");
+        }
+
+        int failed_bc = BCCheckErrors(result_pair.first, dev_node_bc_values.host_vec);
+
+        if (failed_bc)
+        {
+            printf("BC value failed!\n");
+        }
+        return failed_sigmas + failed_bc == 0;
+    }
+    else
+    {
+        printf("Warning: Result not checked\n");
+        return true;
+    }
+
+}
diff --git a/samples/bc/bc_common.h b/samples/bc/bc_common.h
new file mode 100644
index 0000000..5d19723
--- /dev/null
+++ b/samples/bc/bc_common.h
@@ -0,0 +1,50 @@
+// Groute: An Asynchronous Multi-GPU Programming Framework
+// http://www.github.com/groute/groute
+// Copyright (c) 2017, A. Barak
+// All rights reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+// * Redistributions of source code must retain the above copyright notice,
+//   this list of conditions and the following disclaimer.
+// * Redistributions in binary form must reproduce the above copyright notice,
+//   this list of conditions and the following disclaimer in the documentation
+//   and/or other materials provided with the distribution.
+// * Neither the names of the copyright holders nor the names of its 
+//   contributors may be used to endorse or promote products derived from this
+//   software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
+#ifndef __BC_COMMON_H
+#define __BC_COMMON_H
+
+#include <utils/stopwatch.h>
+#include <unordered_set>
+#include <vector>
+
+#include <groute/graphs/csr_graph.h>
+
+typedef uint32_t level_t;
+typedef float centrality_t;
+typedef float sigma_t;
+#define ERROR_THRESHOLD 0.05
+
+std::pair<std::vector<centrality_t>, std::vector<sigma_t >>
+BetweennessCentralityHost(const groute::graphs::host::CSRGraph &graph, index_t src);
+
+int BCCheckErrors(std::vector<float> &regression_bc_values, std::vector<float> &bc_values);
+
+int BCOutput(const char *file, const std::vector<centrality_t> &bc_values);
+
+#endif
diff --git a/samples/bc/bc_host.cpp b/samples/bc/bc_host.cpp
new file mode 100644
index 0000000..016b3c6
--- /dev/null
+++ b/samples/bc/bc_host.cpp
@@ -0,0 +1,183 @@
+// Groute: An Asynchronous Multi-GPU Programming Framework
+// http://www.github.com/groute/groute
+// Copyright (c) 2017, A. Barak
+// All rights reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+// * Redistributions of source code must retain the above copyright notice,
+//   this list of conditions and the following disclaimer.
+// * Redistributions in binary form must reproduce the above copyright notice,
+//   this list of conditions and the following disclaimer in the documentation
+//   and/or other materials provided with the distribution.
+// * Neither the names of the copyright holders nor the names of its 
+//   contributors may be used to endorse or promote products derived from this
+//   software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
+#include <queue>
+
+#include "bc_common.h"
+
+DECLARE_bool(verbose);
+
+std::pair<std::vector<centrality_t>, std::vector<sigma_t >> BetweennessCentralityHost(const groute::graphs::host::CSRGraph &graph,
+                                                                                      index_t src)
+{
+    Stopwatch sw(true);
+
+    std::vector<int> source_path(graph.nnodes, -1);
+    std::vector<centrality_t> bc_values(graph.nnodes, 0.0);
+    std::vector<sigma_t> sigmas(graph.nnodes, 0.0);
+
+    source_path[src] = 0;
+    int search_depth = 0;
+    sigmas[src] = 1;
+
+    std::queue<index_t> wl1, wl2;
+    std::queue<index_t> *in_wl = &wl1, *out_wl = &wl2;
+
+    in_wl->push(src);
+
+
+    while (!in_wl->empty())
+    {
+        while (!in_wl->empty())
+        {
+            index_t node = in_wl->front();
+            in_wl->pop();
+
+            int nbr_dist = source_path[node] + 1;
+
+            for (index_t edge = graph.begin_edge(node),
+                         end_edge = graph.end_edge(node); edge < end_edge; edge++)
+            {
+                index_t dest = graph.edge_dest(edge);
+
+                if (source_path[dest] == -1)
+                {
+                    source_path[dest] = nbr_dist;
+                    sigmas[dest] += sigmas[node];
+
+                    if (search_depth < nbr_dist)
+                    {
+                        search_depth = nbr_dist;
+                    }
+
+                    out_wl->push(dest);
+                }
+                else
+                {
+                    if (source_path[dest] == source_path[node] + 1)
+                    {
+                        sigmas[dest] += sigmas[node];
+                    }
+                }
+            }
+        }
+        std::swap(in_wl, out_wl);
+    }
+    search_depth++;
+
+    for (int iter = search_depth - 2; iter > 0; --iter)
+    {
+        for (index_t node = 0; node < graph.nnodes; node++)
+        {
+            if (source_path[node] == iter)
+            {
+                for (index_t edge = graph.begin_edge(node),
+                             end_edge = graph.end_edge(node); edge < end_edge; edge++)
+                {
+                    index_t dest = graph.edge_dest(edge);
+
+                    if (source_path[dest] == iter + 1)
+                    {
+                        bc_values[node] += 1.0f * sigmas[node] / sigmas[dest] *
+                                           (1.0f + bc_values[dest]);
+                    }
+                }
+            }
+        }
+    }
+
+    for (index_t node = 0; node < graph.nnodes; node++)
+    {
+        bc_values[node] *= 0.5f;
+    }
+    sw.stop();
+
+    if (FLAGS_verbose)
+    {
+        printf("\nBC Host: %f ms. \n", sw.ms());
+    }
+
+    return std::make_pair(bc_values, sigmas);
+}
+
+int BCCheckErrors(std::vector<float> &regression_bc_values, std::vector<float> &bc_values) {
+    if (bc_values.size() != regression_bc_values.size()) {
+        return std::abs((int64_t) bc_values.size() - (int64_t) regression_bc_values.size());
+    }
+
+    index_t nnodes = bc_values.size();
+    int total_errors = 0;
+
+    for (index_t node = 0; node < nnodes; node++) {
+
+        bool is_right = true;
+
+        if (fabs(bc_values[node] - 0.0) < 0.01f) {
+            if (fabs(bc_values[node] - regression_bc_values[node]) > ERROR_THRESHOLD) {
+                is_right = false;
+            }
+        } else {
+            if (fabs((bc_values[node] - regression_bc_values[node]) / regression_bc_values[node]) > ERROR_THRESHOLD) {
+                is_right = false;
+            }
+        }
+
+        if (!is_right) {
+            fprintf(stderr, "node: %u bc regression: %f device: %f\n", node,
+                    regression_bc_values[node],
+                    bc_values[node]);
+            total_errors++;
+        }
+    }
+
+    printf("Total errors: %u\n", total_errors);
+
+    return total_errors;
+}
+
+int BCOutput(const char *file, const std::vector<centrality_t> &bc_values)
+{
+    FILE *f;
+    f = fopen(file, "w");
+
+    if (f)
+    {
+        for (int i = 0; i < bc_values.size(); ++i)
+        {
+            fprintf(f, "%u %f\n", i, bc_values[i]);
+        }
+        fclose(f);
+
+        return 1;
+    }
+    else
+    {
+        fprintf(stderr, "Could not open '%s' for writing\n", file);
+        return 0;
+    }
+}
diff --git a/samples/bc/main.cpp b/samples/bc/main.cpp
new file mode 100644
index 0000000..4c49f5c
--- /dev/null
+++ b/samples/bc/main.cpp
@@ -0,0 +1,75 @@
+// Groute: An Asynchronous Multi-GPU Programming Framework
+// http://www.github.com/groute/groute
+// Copyright (c) 2017, A. Barak
+// All rights reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+// * Redistributions of source code must retain the above copyright notice,
+//   this list of conditions and the following disclaimer.
+// * Redistributions in binary form must reproduce the above copyright notice,
+//   this list of conditions and the following disclaimer in the documentation
+//   and/or other materials provided with the distribution.
+// * Neither the names of the copyright holders nor the names of its 
+//   contributors may be used to endorse or promote products derived from this
+//   software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
+#include <cstdio>
+#include <cuda_runtime.h>
+#include <gflags/gflags.h>
+
+#include <utils/utils.h>
+#include <utils/app_skeleton.h>
+
+bool TestBCSingle();
+
+void CleanupGraphs();
+
+namespace bfs
+{
+    struct App
+    {
+        static const char* Name()       { return "bc"; }
+        static const char* NameUpper()  { return "BC"; }
+
+        static bool Single()            { return TestBCSingle(); }
+
+        static bool AsyncMulti(int G)
+        {
+            exit(1);
+            return false;
+        }
+
+        static void Cleanup()           { CleanupGraphs(); }
+    };
+}
+
+int main(int argc, char **argv)
+{
+    Skeleton<bfs::App> app;
+    int exit = app(argc, argv);
+
+    // cudaDeviceReset must be called before exiting in order for profiling and
+    // tracing tools such as Nsight and Visual Profiler to show complete traces.
+    cudaError_t cudaStatus = cudaDeviceReset();
+    if (cudaStatus != cudaSuccess) {
+        fprintf(stderr, "cudaDeviceReset failed!");
+        return 1;
+    }
+
+    return exit;
+}
+
+
diff --git a/samples/pr/main.cpp b/samples/pr/main.cpp
index e312620..18278b7 100644
--- a/samples/pr/main.cpp
+++ b/samples/pr/main.cpp
@@ -36,10 +36,14 @@
 #include <utils/interactor.h>
 #include <utils/app_skeleton.h>
 
-
+DEFINE_double(error, 0.01, "PR error");
+DEFINE_bool(wl_sort, false, "sort worklist by node id");
 bool TestPageRankSingle();
+
 bool TestPageRankAsyncMulti(int ngpus);
+
 bool TestPageRankAsyncMultiOptimized(int ngpus);
+
 void CleanupGraphs();
 
 
@@ -47,13 +51,20 @@ namespace pr
 {
     struct App
     {
-        static const char* Name()       { return "page rank"; }
-        static const char* NameUpper()  { return "Page Rank"; }
+        static const char *Name()
+        { return "page rank"; }
 
-        static bool Single()            { return TestPageRankSingle(); }
-        static bool AsyncMulti(int G)   { return FLAGS_opt ? TestPageRankAsyncMultiOptimized(G) : TestPageRankAsyncMulti(G); }
+        static const char *NameUpper()
+        { return "Page Rank"; }
 
-        static void Cleanup()           { CleanupGraphs(); }
+        static bool Single()
+        { return TestPageRankSingle(); }
+
+        static bool AsyncMulti(int G)
+        { return FLAGS_opt ? TestPageRankAsyncMultiOptimized(G) : TestPageRankAsyncMulti(G); }
+
+        static void Cleanup()
+        { CleanupGraphs(); }
     };
 }
 
@@ -65,7 +76,8 @@ int main(int argc, char **argv)
     // cudaDeviceReset must be called before exiting in order for profiling and
     // tracing tools such as Nsight and Visual Profiler to show complete traces.
     cudaError_t cudaStatus = cudaDeviceReset();
-    if (cudaStatus != cudaSuccess) {
+    if (cudaStatus != cudaSuccess)
+    {
         fprintf(stderr, "cudaDeviceReset failed!");
         return 1;
     }
diff --git a/samples/pr/pr_async.cu b/samples/pr/pr_async.cu
index 0d2d9d8..c61f01d 100644
--- a/samples/pr/pr_async.cu
+++ b/samples/pr/pr_async.cu
@@ -31,7 +31,8 @@
 #include <thread>
 #include <memory>
 #include <random>
-
+#include <thrust/sort.h>
+#include <thrust/device_ptr.h>
 #include <gflags/gflags.h>
 
 #include <groute/event_pool.h>
@@ -49,7 +50,8 @@
 
 DECLARE_int32(max_pr_iterations);
 DECLARE_bool(verbose);
-
+DECLARE_bool(wl_sort);
+DECLARE_double(error);
 #define GTID (blockIdx.x * blockDim.x + threadIdx.x)
 
 
@@ -153,8 +155,8 @@ namespace pr
     __global__ void PageRankKernel__Single__NestedParallelism__(
         TGraph graph,
         RankDatum<rank_t> current_ranks, ResidualDatum<rank_t> residual,
-        WorkSource work_source, TWorklist<index_t> output_worklist
-        )
+        WorkSource work_source, TWorklist<index_t> output_worklist,
+        double error)
     {
         unsigned tid = TID_1D;
         unsigned nthreads = TOTAL_THREADS_1D;
@@ -188,11 +190,11 @@ namespace pr
 
             groute::dev::CTAWorkScheduler<rank_t>::template schedule(
                 np_local, 
-                [&graph, &residual, &output_worklist](index_t edge, rank_t update)
+                [&graph, &residual, &output_worklist, &error](index_t edge, rank_t update)
                 {
                     index_t dest = graph.edge_dest(edge);
                     rank_t prev = atomicAdd(residual.get_item_ptr(dest), update);
-                    if (prev + update > EPSILON && prev < EPSILON)
+                    if (prev + update > error && prev < error)
                     {
                         output_worklist.append_warp(dest);
                     }
@@ -209,8 +211,8 @@ namespace pr
     __global__ void PageRankKernel__Single__(
         TGraph graph,
         RankDatum<rank_t> current_ranks, ResidualDatum<rank_t> residual,
-        WorkSource work_source, TWorklist<index_t> output_worklist
-        )
+        WorkSource work_source, TWorklist<index_t> output_worklist,
+        double error)
     {
         unsigned tid = TID_1D;
         unsigned nthreads = TOTAL_THREADS_1D;
@@ -239,7 +241,7 @@ namespace pr
             {
                 index_t dest = graph.edge_dest(edge);
                 rank_t prev = atomicAdd(residual.get_item_ptr(dest), update);
-                if (prev + update > EPSILON && prev < EPSILON)
+                if (prev + update > error && prev < error)
                 {
                     output_worklist.append_warp(dest);
                 }
@@ -260,8 +262,8 @@ namespace pr
         ResidualDatum<rank_t> residual,
         WorkSource work_source, 
         TWorklist<index_t> local_output_worklist,
-        WorkTarget remote_work_target
-        )
+        WorkTarget remote_work_target,
+        double error)
     {
         unsigned tid = TID_1D;
         unsigned nthreads = TOTAL_THREADS_1D;
@@ -295,14 +297,14 @@ namespace pr
 
             groute::dev::CTAWorkScheduler<rank_t>::template schedule(
                 np_local, 
-                [&graph, &residual, &local_output_worklist, &remote_work_target](index_t edge, rank_t update)
+                [&graph, &residual, &local_output_worklist, &remote_work_target, &error](index_t edge, rank_t update)
                 {
                     index_t dest = graph.edge_dest(edge);
                     rank_t prev = atomicAdd(residual.get_item_ptr(dest), update);
 
                     if (graph.owns(dest))
                     {
-                        if (prev + update > EPSILON && prev <= EPSILON)
+                        if (prev + update > error && prev <= error)
                         {
                             local_output_worklist.append_warp(dest);
                         }
@@ -333,8 +335,8 @@ namespace pr
         ResidualDatum<rank_t> residual,
         WorkSource work_source, 
         TWorklist<index_t> local_output_worklist,
-        WorkTarget remote_work_target
-        )
+        WorkTarget remote_work_target,
+        double error)
     {
         unsigned tid = TID_1D;
         unsigned nthreads = TOTAL_THREADS_1D;
@@ -366,7 +368,7 @@ namespace pr
 
                 if (graph.owns(dest))
                 {
-                    if (prev + update > EPSILON && prev <= EPSILON)
+                    if (prev + update > error && prev <= error)
                     {
                         local_output_worklist.append_warp(dest);
                     }
@@ -388,7 +390,7 @@ namespace pr
     private:
         groute::graphs::dev::CSRGraphSeg m_graph_seg;
         groute::graphs::dev::GraphDatum<rank_t> m_residual;
-
+        double error;
     public:
         template<typename...UnusedData>
         SplitOps(
@@ -400,6 +402,7 @@ namespace pr
             m_graph_seg(graph_seg),
             m_residual(residual)
         {
+            error = FLAGS_error;
         }
 
         __device__ __forceinline__ groute::SplitFlags on_receive(const remote_work_t& work)
@@ -407,7 +410,7 @@ namespace pr
             if (m_graph_seg.owns(work.node))
             {
                 rank_t prev = atomicAdd(m_residual.get_item_ptr(work.node), work.rank);
-                return (prev + work.rank > EPSILON && prev < EPSILON) 
+                return (prev + work.rank > error && prev < error)
                     ? groute::SF_Take
                     : groute::SF_None;
             }
@@ -518,7 +521,8 @@ namespace pr
                 PageRankKernel__Single__NestedParallelism__ << < grid_dims, block_dims, 0, stream.cuda_stream >> >(
                     m_graph, m_current_ranks, m_residual,
                     work_source,
-                    output_worklist.DeviceObject());
+                    output_worklist.DeviceObject(),
+                    FLAGS_error);
             }
             else
             {
@@ -526,7 +530,8 @@ namespace pr
                 PageRankKernel__Single__ << < grid_dims, block_dims, 0, stream.cuda_stream >> >(
                     m_graph, m_current_ranks, m_residual,
                     work_source,
-                    output_worklist.DeviceObject());
+                    output_worklist.DeviceObject(),
+                    FLAGS_error);
             }
         }
 
@@ -545,7 +550,8 @@ namespace pr
                     m_graph, m_current_ranks, m_residual,
                     work_source,
                     output_worklist.DeviceObject(),
-                    WorkTargetWorklist(output_worklist));
+                    WorkTargetWorklist(output_worklist),
+                    FLAGS_error);
             }
             else
             {
@@ -554,7 +560,8 @@ namespace pr
                     m_graph, m_current_ranks, m_residual,
                     work_source,
                     output_worklist.DeviceObject(),
-                    WorkTargetWorklist(output_worklist));
+                    WorkTargetWorklist(output_worklist),
+                    FLAGS_error);
             }
         }
     };
@@ -696,7 +703,17 @@ namespace pr
             UnusedData&... data)
         {
             graph_allocator.GatherDatum(current_ranks);
-            return current_ranks.GetHostData();
+
+            auto host_data = current_ranks.GetHostData();
+            double pr_sum = 0;
+
+            for (auto pr:host_data){
+                pr_sum += pr;
+            }
+
+            printf("Total rank : %f\n", pr_sum);
+
+            return host_data;
         }
 
         template<
@@ -785,28 +802,51 @@ bool TestPageRankSingle()
     Stopwatch sw(true);
 
     groute::Worklist<index_t>* in_wl = &wl1, *out_wl = &wl2;
+    int iteration = 0;
+    float sort_time = 0;
 
     solver.Init__Single__(stream);
-    
+
+    Stopwatch sw_iter(true);
     // First relax is a special case, starts from all owned nodes
     solver.Relax__Single__( 
         groute::dev::WorkSourceRange<index_t>(
             dev_graph_allocator.DeviceObject().owned_start_node(), 
             dev_graph_allocator.DeviceObject().owned_nnodes()), 
             *in_wl, stream);
+    stream.Sync();
+    sw_iter.stop();
+
+    printf("Iteration: %d Time: %f ms\n", iteration++, sw_iter.ms());
 
     groute::Segment<index_t> work_seg;
     work_seg = in_wl->ToSeg(stream);
 
-    int iteration = 0;
+
 
     while (work_seg.GetSegmentSize() > 0)
     {
+        sw_iter.start();
+
+
+        if (FLAGS_wl_sort) {
+            Stopwatch sw_sort(true);
+            thrust::device_ptr<index_t> d_worklist(work_seg.GetSegmentPtr());
+            thrust::sort(d_worklist, d_worklist + work_seg.GetSegmentSize());
+            sw_sort.stop();
+
+            sort_time += sw_sort.ms();
+        }
+
         solver.Relax__Single__(
             groute::dev::WorkSourceArray<index_t>(
                 work_seg.GetSegmentPtr(), 
                 work_seg.GetSegmentSize()), 
             *out_wl, stream);
+        stream.Sync();
+        sw_iter.stop();
+
+        printf("Iteration: %d Time: %f ms\n", iteration, sw_iter.ms());
 
         if (++iteration > FLAGS_max_pr_iterations) break;
 
@@ -821,11 +861,20 @@ bool TestPageRankSingle()
         printf("\nWarning: ignoring repetitions flag, running just one repetition (not implemented)\n");
 
     printf("\n%s: %f ms. <filter>\n\n", pr::Algo::Name(), sw.ms() / FLAGS_repetitions);
+    printf("Sort: %f ms.\n\n", sort_time);
     printf("%s terminated after %d iterations (max: %d)\n\n", pr::Algo::Name(), iteration, FLAGS_max_pr_iterations);
 
     // Gather
     auto gathered_output = pr::Algo::Gather(dev_graph_allocator, residual, current_ranks);
 
+    double pr_sum = 0;
+
+    for (auto pr:gathered_output){
+        pr_sum += pr;
+    }
+
+    printf("Total rank : %f\n", pr_sum);
+
     if (FLAGS_output.length() != 0)
         pr::Algo::Output(FLAGS_output.c_str(), gathered_output);
 
diff --git a/samples/pr/pr_host.cpp b/samples/pr/pr_host.cpp
index d8364ce..407367e 100644
--- a/samples/pr/pr_host.cpp
+++ b/samples/pr/pr_host.cpp
@@ -33,7 +33,7 @@
 DEFINE_int32(max_pr_iterations, 200, "The maximum number of PR iterations"); // used just for host and some single versions  
 DEFINE_int32(top_ranks, 10, "The number of top ranks to compare for PR regression");
 DEFINE_bool(print_ranks, false, "Write out ranks to output");
-
+DECLARE_double(error);
 
 std::vector<rank_t> PageRankHost(groute::graphs::host::CSRGraph& graph)
 {
@@ -97,7 +97,7 @@ std::vector<rank_t> PageRankHost(groute::graphs::host::CSRGraph& graph)
                 rank_t prev = residual[dest];
                 residual[dest] += update;
 
-                if (prev + update > EPSILON && prev < EPSILON)
+                if (prev + update > FLAGS_error && prev < FLAGS_error)
                 {
                     out_wl->push(dest);
                 }
@@ -213,13 +213,17 @@ int PageRankOutput(const char *file, const std::vector<rank_t>& ranks)
         std::stable_sort(pr, pr + ranks.size());
         fprintf(stderr, "Writing to file ...\n");
 
-        fprintf(f, "ALPHA %*e EPSILON %*e\n", FLT_DIG, ALPHA, FLT_DIG, EPSILON);
-        fprintf(f, "RANKS 1--%d of %d\n", FLAGS_top_ranks, (int)ranks.size());
-        for (int i = 1; i <= FLAGS_top_ranks; i++) {
+        int top_ranks = ranks.size();
+
+        fprintf(f, "ALPHA %*e EPSILON %*e\n", FLT_DIG, ALPHA, FLT_DIG, FLAGS_error);
+        fprintf(f, "RANKS 1--%d of %d\n", top_ranks, (int)ranks.size());
+        for (int i = 1; i <= top_ranks; i++) {
             if (!FLAGS_print_ranks)
                 fprintf(f, "%d %d\n", i, pr[ranks.size() - i].node);
             else
-                fprintf(f, "%d %d %*e\n", i, pr[ranks.size() - i].node, FLT_DIG, pr[ranks.size() - i].rank / sum);
+//                fprintf(f, "%d %d %*e\n", i, pr[ranks.size() - i].node, FLT_DIG, pr[ranks.size() - i].rank);
+//                fprintf(f, "%d %*e\n", pr[ranks.size() - i].node, FLT_DIG, pr[ranks.size() - i].rank / sum);
+                fprintf(f, "%d %*e\n", pr[ranks.size() - i].node, FLT_DIG, pr[ranks.size() - i].rank);
         }
 
         free(pr);
